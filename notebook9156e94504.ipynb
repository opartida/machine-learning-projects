{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Welcome to the year 2912, where your data science skills are needed to solve a cosmic mystery. We've received a transmission from four lightyears away and things aren't looking good.\n\nThe Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars.\n\nWhile rounding Alpha Centauri en route to its first destination—the torrid 55 Cancri E—the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!\n\n\n\nTo help rescue crews and retrieve the lost passengers, you are challenged to predict which passengers were transported by the anomaly using records recovered from the spaceship’s damaged computer system.\n\nHelp save them and change history!","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Dropout, Normalization\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.data import Dataset\nfrom tensorflow import feature_column\nimport matplotlib.pyplot as plt\n# pip install seaborn\nimport seaborn as sns\nimport re\nimport math\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n%config InlineBackend.figure_format = 'retina'\npd.set_option(\"display.precision\", 2)\nBASE_DIR = '/kaggle/input/spaceship-titanic'\nBASE_DIR_OUTPUT = '/kaggle/working/'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Facets","metadata":{}},{"cell_type":"code","source":"# For facets\nfrom IPython.core.display import display, HTML\nimport base64\n!pip install facets-overview\nfrom facets_overview.feature_statistics_generator import FeatureStatisticsGenerator\n\ndef displayFacetStatistics(df):\n    fsg = FeatureStatisticsGenerator()\n    dataframes = [\n        {'table': df, 'name': 'trainData'}]\n    spaceshipProto = fsg.ProtoFromDataFrames(dataframes)\n    protostr = base64.b64encode(spaceshipProto.SerializeToString()).decode(\"utf-8\")\n\n    HTML_TEMPLATE = \"\"\"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n            <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\">\n            <facets-overview id=\"elem\"></facets-overview>\n            <script>\n              document.querySelector(\"#elem\").protoInput = \"{protostr}\";\n            </script>\"\"\"\n    html = HTML_TEMPLATE.format(protostr=protostr)\n    display(HTML(html))\n    \ndef displayFacetsDive(df):\n    SAMPLE_SIZE = len(df.index) - 1\n    df.dropna(how=\"any\", axis=0, inplace=True)\n    train_dive = df.to_json(orient='records')\n    HTML_TEMPLATE = \"\"\"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n            <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\">\n            <facets-dive id=\"elem\" height=\"600\"></facets-dive>\n            <script>\n              var data = {jsonstr};\n              document.querySelector(\"#elem\").data = data;\n            </script>\"\"\"\n    html = HTML_TEMPLATE.format(jsonstr=train_dive)\n    display(HTML(html))","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"ALL_COLUMNS = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck','Destination', 'HomePlanet', 'Cabin','CryoSleep', 'Transported']\nLABEL_COLUMN = ['Transported']\nCATEGORICAL_COLUMNS = ['Destination', 'HomePlanet', 'Cabin']\nBOOLEAN_COLUMNS = ['CryoSleep']\nDROP_COLUMNS = ['Name' , 'VIP']\nNUMERICAL_COLUMNS = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\nAMENITY_COLUMNS = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']    \n\ndef createModel(feature_layer):\n    class myModel(tf.keras.Model):\n        def __init__(self):\n            super(myModel, self).__init__()\n            self.feature_layer = feature_layer\n            self.normalization = Normalization(mean=0, variance=1)\n            self.dense2 = Dense(16, activation='relu')         \n            self.dense3 = Dense(1, activation='sigmoid')\n            self.dropout1 = Dropout(0.4)\n\n        def call(self, inputs, training=False):\n            print(inputs)\n            x = self.feature_layer(inputs)\n            x = self.normalization(x)\n            #x = self.dense1(x)            \n            x = self.dense2(x)   \n            if(training==False):\n                x = self.dropout1(x, training=training)\n            return self.dense3(x)\n\n    model = myModel()       \n\n    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), \n                  loss=tf.keras.losses.BinaryCrossentropy(), \n                  metrics=['accuracy'])\n    \n    return model\n\ndef train_model(model, train_ds):\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=10)  \n    history = model.fit(train_ds, epochs= 100, callbacks=[early_stopping])\n    model.save(os.path.join(BASE_DIR_OUTPUT, 'spaceship_model'), save_format='tf')\n\ndef make_predictions(ds):\n    \n    model = load_model(os.path.join(BASE_DIR_OUTPUT, 'spaceship_model'))\n    predictions = model.predict(ds)\n    predictions = np.round(predictions)\n    predictions = [True if pred == 1 else False for pred in predictions]\n    \n    passenger_id = np.concatenate(np.array([x['PassengerId'].numpy() for x in ds]), axis=0)\n    passenger_id = np.array([x.decode(\"utf-8\") for x in passenger_id])\n    passenger_id = np.expand_dims(passenger_id, axis=1)\n    preds = np.array(predictions, ndmin=2)\n    \n    preds = np.concatenate((passenger_id, preds.T), axis=1)\n    df = pd.DataFrame(preds)\n    predictions_file = os.path.join(BASE_DIR_OUTPUT, 'predictions.csv')\n    \n    if(os.path.exists(predictions_file)):\n        os.remove(predictions_file)\n            \n    df.to_csv(path_or_buf=predictions_file, header=['PassengerId', 'Transported'], index=False)\n        \ndef df_to_dataset(dataframe, shuffle=True, batch_size=32, test=False):\n    dataframe = dataframe.copy()    \n    if(not test):\n        labels = dataframe.pop(LABEL_COLUMN[0])\n        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    else:\n        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe)))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    return ds     \n\ndef fill_na(df):\n    values = {col: df[col].value_counts().idxmax() for col in df.columns}\n    df.fillna(values, inplace=True)\n    return df\n\ndef createFeatureColumns():\n    feature_columns = []\n    # numeric cols\n    for col in ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']:\n        feature_columns.append(feature_column.numeric_column(col))\n\n    # bucketized cols\n    age = feature_column.numeric_column('Age')\n    age_buckets = feature_column.bucketized_column(age, boundaries=[0, 20, 40])\n    feature_columns.append(age_buckets)\n\n    # indicator_columns\n    indicator_column_names = ['HomePlanet', 'Destination']\n    for col_name in indicator_column_names:\n        categorical_column = feature_column.categorical_column_with_vocabulary_list(\n            col_name, train_df[col_name].unique())\n        indicator_column = feature_column.indicator_column(categorical_column)\n        feature_columns.append(indicator_column)\n\n    # embedding columns\n    cabin = feature_column.categorical_column_with_vocabulary_list(\n        'Cabin', train_df.Cabin.unique())\n    cabin_embedding = feature_column.embedding_column(cabin, dimension=8)\n    feature_columns.append(cabin_embedding)\n    return feature_columns","metadata":{"execution":{"iopub.status.busy":"2022-07-23T21:16:34.242389Z","iopub.execute_input":"2022-07-23T21:16:34.244795Z","iopub.status.idle":"2022-07-23T21:16:34.274801Z","shell.execute_reply.started":"2022-07-23T21:16:34.244741Z","shell.execute_reply":"2022-07-23T21:16:34.273322Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nBASE_DIR = '/kaggle/input/spaceship-titanic'\ntrain_df = pd.read_csv(os.path.join(BASE_DIR, 'train.csv'), index_col=0)\ntest_df = pd.read_csv(os.path.join(BASE_DIR, 'test.csv'), index_col=0)\ntrain_df = train_df.reset_index()\ntest_df = test_df.reset_index()\ndisplay(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-07-23T21:13:02.299878Z","iopub.execute_input":"2022-07-23T21:13:02.300631Z","iopub.status.idle":"2022-07-23T21:13:02.382332Z","shell.execute_reply.started":"2022-07-23T21:13:02.300597Z","shell.execute_reply":"2022-07-23T21:13:02.380842Z"},"trusted":true},"execution_count":125,"outputs":[{"output_type":"display_data","data":{"text/plain":"     PassengerId HomePlanet CryoSleep     Cabin    Destination   Age    VIP  \\\n0        0001_01     Europa     False     B/0/P    TRAPPIST-1e  39.0  False   \n1        0002_01      Earth     False     F/0/S    TRAPPIST-1e  24.0  False   \n2        0003_01     Europa     False     A/0/S    TRAPPIST-1e  58.0   True   \n3        0003_02     Europa     False     A/0/S    TRAPPIST-1e  33.0  False   \n4        0004_01      Earth     False     F/1/S    TRAPPIST-1e  16.0  False   \n...          ...        ...       ...       ...            ...   ...    ...   \n8688     9276_01     Europa     False    A/98/P    55 Cancri e  41.0   True   \n8689     9278_01      Earth      True  G/1499/S  PSO J318.5-22  18.0  False   \n8690     9279_01      Earth     False  G/1500/S    TRAPPIST-1e  26.0  False   \n8691     9280_01     Europa     False   E/608/S    55 Cancri e  32.0  False   \n8692     9280_02     Europa     False   E/608/S    TRAPPIST-1e  44.0  False   \n\n      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n0             0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n1           109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n2            43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n3             0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n4           303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n...           ...        ...           ...     ...     ...                ...   \n8688          0.0     6819.0           0.0  1643.0    74.0  Gravior Noxnuther   \n8689          0.0        0.0           0.0     0.0     0.0    Kurta Mondalley   \n8690          0.0        0.0        1872.0     1.0     0.0       Fayey Connon   \n8691          0.0     1049.0           0.0   353.0  3235.0   Celeon Hontichre   \n8692        126.0     4688.0           0.0     0.0    12.0   Propsh Hontichre   \n\n      Transported  \n0           False  \n1            True  \n2           False  \n3           False  \n4            True  \n...           ...  \n8688        False  \n8689        False  \n8690         True  \n8691        False  \n8692         True  \n\n[8693 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>HomePlanet</th>\n      <th>CryoSleep</th>\n      <th>Cabin</th>\n      <th>Destination</th>\n      <th>Age</th>\n      <th>VIP</th>\n      <th>RoomService</th>\n      <th>FoodCourt</th>\n      <th>ShoppingMall</th>\n      <th>Spa</th>\n      <th>VRDeck</th>\n      <th>Name</th>\n      <th>Transported</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0001_01</td>\n      <td>Europa</td>\n      <td>False</td>\n      <td>B/0/P</td>\n      <td>TRAPPIST-1e</td>\n      <td>39.0</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Maham Ofracculy</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0002_01</td>\n      <td>Earth</td>\n      <td>False</td>\n      <td>F/0/S</td>\n      <td>TRAPPIST-1e</td>\n      <td>24.0</td>\n      <td>False</td>\n      <td>109.0</td>\n      <td>9.0</td>\n      <td>25.0</td>\n      <td>549.0</td>\n      <td>44.0</td>\n      <td>Juanna Vines</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0003_01</td>\n      <td>Europa</td>\n      <td>False</td>\n      <td>A/0/S</td>\n      <td>TRAPPIST-1e</td>\n      <td>58.0</td>\n      <td>True</td>\n      <td>43.0</td>\n      <td>3576.0</td>\n      <td>0.0</td>\n      <td>6715.0</td>\n      <td>49.0</td>\n      <td>Altark Susent</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0003_02</td>\n      <td>Europa</td>\n      <td>False</td>\n      <td>A/0/S</td>\n      <td>TRAPPIST-1e</td>\n      <td>33.0</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1283.0</td>\n      <td>371.0</td>\n      <td>3329.0</td>\n      <td>193.0</td>\n      <td>Solam Susent</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0004_01</td>\n      <td>Earth</td>\n      <td>False</td>\n      <td>F/1/S</td>\n      <td>TRAPPIST-1e</td>\n      <td>16.0</td>\n      <td>False</td>\n      <td>303.0</td>\n      <td>70.0</td>\n      <td>151.0</td>\n      <td>565.0</td>\n      <td>2.0</td>\n      <td>Willy Santantines</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8688</th>\n      <td>9276_01</td>\n      <td>Europa</td>\n      <td>False</td>\n      <td>A/98/P</td>\n      <td>55 Cancri e</td>\n      <td>41.0</td>\n      <td>True</td>\n      <td>0.0</td>\n      <td>6819.0</td>\n      <td>0.0</td>\n      <td>1643.0</td>\n      <td>74.0</td>\n      <td>Gravior Noxnuther</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>8689</th>\n      <td>9278_01</td>\n      <td>Earth</td>\n      <td>True</td>\n      <td>G/1499/S</td>\n      <td>PSO J318.5-22</td>\n      <td>18.0</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Kurta Mondalley</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>8690</th>\n      <td>9279_01</td>\n      <td>Earth</td>\n      <td>False</td>\n      <td>G/1500/S</td>\n      <td>TRAPPIST-1e</td>\n      <td>26.0</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1872.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>Fayey Connon</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>8691</th>\n      <td>9280_01</td>\n      <td>Europa</td>\n      <td>False</td>\n      <td>E/608/S</td>\n      <td>55 Cancri e</td>\n      <td>32.0</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1049.0</td>\n      <td>0.0</td>\n      <td>353.0</td>\n      <td>3235.0</td>\n      <td>Celeon Hontichre</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>8692</th>\n      <td>9280_02</td>\n      <td>Europa</td>\n      <td>False</td>\n      <td>E/608/S</td>\n      <td>TRAPPIST-1e</td>\n      <td>44.0</td>\n      <td>False</td>\n      <td>126.0</td>\n      <td>4688.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>12.0</td>\n      <td>Propsh Hontichre</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>8693 rows × 14 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"code","source":"df = train_df.drop('Transported', axis=1)\ndisplayFacetStatistics(df)\n\n#displayFacetsDive(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess Data","metadata":{}},{"cell_type":"code","source":"BASE_DIR = '/kaggle/input/spaceship-titanic'\ntrain_df = pd.read_csv(os.path.join(BASE_DIR, 'train.csv'), index_col=0)\ntest_df = pd.read_csv(os.path.join(BASE_DIR, 'test.csv'), index_col=0)\ntrain_df = train_df.reset_index()\ntest_df = test_df.reset_index()\n# with tf.dataset\nbatch_size = 100\n#train_df = fill_na(train_df)\n#test_df = fill_na(test_df)\ntrain_df[BOOLEAN_COLUMNS] = [1 if x == True else 0 for x in train_df[BOOLEAN_COLUMNS]]\ntrain_df.drop(columns=DROP_COLUMNS, inplace=True)\nvalues={'Cabin': '', 'Destination': '', 'HomePlanet': ''}\ntrain_df = train_df.fillna(values)\ndisplay(train_df.info())\ntest_df = test_df.fillna(method='ffill')\n\ntrain_ds = df_to_dataset(train_df, batch_size=batch_size)\n#test_ds = df_to_dataset(test_df, test=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-23T21:18:56.090033Z","iopub.execute_input":"2022-07-23T21:18:56.090490Z","iopub.status.idle":"2022-07-23T21:18:56.198695Z","shell.execute_reply.started":"2022-07-23T21:18:56.090455Z","shell.execute_reply":"2022-07-23T21:18:56.197651Z"},"trusted":true},"execution_count":136,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8693 entries, 0 to 8692\nData columns (total 12 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   PassengerId   8693 non-null   object \n 1   HomePlanet    8693 non-null   object \n 2   CryoSleep     8693 non-null   int64  \n 3   Cabin         8693 non-null   object \n 4   Destination   8693 non-null   object \n 5   Age           8514 non-null   float64\n 6   RoomService   8512 non-null   float64\n 7   FoodCourt     8510 non-null   float64\n 8   ShoppingMall  8485 non-null   float64\n 9   Spa           8510 non-null   float64\n 10  VRDeck        8505 non-null   float64\n 11  Transported   8693 non-null   bool   \ndtypes: bool(1), float64(6), int64(1), object(4)\nmemory usage: 755.7+ KB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}}]},{"cell_type":"code","source":"print(next(iter(train_ds))[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create feature columns","metadata":{}},{"cell_type":"code","source":"feature_columns = createFeatureColumns()\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"model = createModel(feature_layer)\ntrain_model(model, train_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_predictions(test_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = feature_layer(next(iter(train_ds))[0])\ndisplay(pd.DataFrame(features.numpy()))\ndisplay(pd.DataFrame(layer(features.numpy()).numpy()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"os.remove(os.path.join(BASE_DIR_OUTPUT, 'predictions.csv'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom matplotlib import pyplot as plt\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pandas_to_numpy(data):\n    '''Convert a pandas DataFrame into a Numpy array'''\n  # Drop empty rows.\n    data = data.dropna(how=\"any\", axis=0)\n  # Separate DataFrame into two Numpy arrays.\n    labels = np.array(data['Transported'])\n    features = data.drop('Transported', axis=1)\n    features = {name:np.array(value) for name, value in features.items()}\n  \n    return features, labels\n\n#@title Visualize Binary Confusion Matrix and Compute Evaluation Metrics Per Subgroup\nCATEGORY  =  \"HomePlanet\" #@param {type:\"string\"}\nSUBGROUP =  1 #@param {type:\"int\"}\n\n# Labels for annotating axes in plot.\nclasses = ['Transported', 'Not Transported']\n\n# Given define subgroup, generate predictions and obtain its corresponding \n# ground truth.\nsubgroup_filter  = train_df.loc[train_df[CATEGORY] == SUBGROUP]\nfeatures, labels = pandas_to_numpy(subgroup_filter)\n\nsubgroup_results = model.evaluate(x=features, y=labels, verbose=0)\ndisplay(subgroup_filter)\nconfusion_matrix = np.array([[subgroup_results[1], subgroup_results[4]], \n                             [subgroup_results[2], subgroup_results[3]]])\n\nsubgroup_performance_metrics = {\n    'ACCURACY': subgroup_results[5],\n    'PRECISION': subgroup_results[6], \n    'RECALL': subgroup_results[7],\n    'AUC': subgroup_results[8]\n}\nperformance_df = pd.DataFrame(subgroup_performance_metrics, index=[SUBGROUP])\npd.options.display.float_format = '{:,.4f}'.format\n\nplot_confusion_matrix(confusion_matrix, classes, SUBGROUP);\nperformance_df\n\ndef plot_confusion_matrix(\n    confusion_matrix, class_names, subgroup, figsize = (8,6)):\n \n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n\n    rcParams.update({\n      'font.family':'sans-serif',\n      'font.sans-serif':['Liberation Sans'],\n    })\n  \n    sns.set_context(\"notebook\", font_scale=1.25)\n\n    fig = plt.figure(figsize=figsize)\n\n    plt.title('Confusion Matrix for Performance Across ' + subgroup)\n\n    # Combine the instance (numercial value) with its description\n    strings = np.asarray([['True Positives', 'False Negatives'],\n                          ['False Positives', 'True Negatives']])\n    labels = (np.asarray(\n        [\"{0:g}\\n{1}\".format(value, string) for string, value in zip(\n            strings.flatten(), confusion_matrix.flatten())])).reshape(2, 2)\n\n    heatmap = sns.heatmap(df_cm, annot=labels, fmt=\"\", \n        linewidths=2.0, cmap=sns.color_palette(\"GnBu_d\"));\n    heatmap.yaxis.set_ticklabels(\n        heatmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n    heatmap.xaxis.set_ticklabels(\n        heatmap.xaxis.get_ticklabels(), rotation=45, ha='right')\n    plt.ylabel('References')\n    plt.xlabel('Predictions')\n    return fig\n\ndef createTrainEvalSets(df):\n    df1 = df[np.mod(np.abs(df['Name'].apply(lambda item: hash(item.split(\" \")[1]))), 4) < 3]\n    df2 = df[np.mod(np.abs(df['Name'].apply(lambda item: hash(item.split(\" \")[1]))), 4) >= 3]\n    return (df1, df2)\n    \ndef displayRowsWithNulls(df, columns):\n    display(df[columns][df[columns].isnull().any(axis=1)])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1, True])\nfor elem in dataset:\n    print(elem.numpy())","metadata":{"execution":{"iopub.status.busy":"2022-07-23T21:12:26.469876Z","iopub.execute_input":"2022-07-23T21:12:26.470346Z","iopub.status.idle":"2022-07-23T21:12:26.486135Z","shell.execute_reply.started":"2022-07-23T21:12:26.470313Z","shell.execute_reply":"2022-07-23T21:12:26.483919Z"},"trusted":true},"execution_count":124,"outputs":[{"name":"stdout","text":"8\n3\n0\n8\n2\n1\n1\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}