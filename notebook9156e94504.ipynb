{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Welcome to the year 2912, where your data science skills are needed to solve a cosmic mystery. We've received a transmission from four lightyears away and things aren't looking good.\n\nThe Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars.\n\nWhile rounding Alpha Centauri en route to its first destination—the torrid 55 Cancri E—the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!\n\n\n\nTo help rescue crews and retrieve the lost passengers, you are challenged to predict which passengers were transported by the anomaly using records recovered from the spaceship’s damaged computer system.\n\nHelp save them and change history!","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.data import Dataset\nimport matplotlib.pyplot as plt\n# pip install seaborn\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, Normalizer\nimport re\nimport math\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n%config InlineBackend.figure_format = 'retina'\npd.set_option(\"display.precision\", 2)\n\n# For facets\nfrom IPython.core.display import display, HTML\nimport base64\n!pip install facets-overview\nfrom facets_overview.feature_statistics_generator import FeatureStatisticsGenerator\n\nBASE_DIR = '/kaggle/input/spaceship-titanic'\nBASE_DIR_OUTPUT = '/kaggle/working/'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"#@title Define Function to Visualize Binary Confusion Matrix\ndef plot_confusion_matrix(\n    confusion_matrix, class_names, subgroup, figsize = (8,6)):\n  # We're taking our calculated binary confusion matrix that's already in the \n  # form of an array and turning it into a pandas DataFrame because it's a lot \n  # easier to work with a pandas DataFrame when visualizing a heat map in \n  # Seaborn.\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n\n    rcParams.update({\n      'font.family':'sans-serif',\n      'font.sans-serif':['Liberation Sans'],\n    })\n  \n    sns.set_context(\"notebook\", font_scale=1.25)\n\n    fig = plt.figure(figsize=figsize)\n\n    plt.title('Confusion Matrix for Performance Across ' + subgroup)\n\n    # Combine the instance (numercial value) with its description\n    strings = np.asarray([['True Positives', 'False Negatives'],\n                          ['False Positives', 'True Negatives']])\n    labels = (np.asarray(\n        [\"{0:g}\\n{1}\".format(value, string) for string, value in zip(\n            strings.flatten(), confusion_matrix.flatten())])).reshape(2, 2)\n\n    heatmap = sns.heatmap(df_cm, annot=labels, fmt=\"\", \n        linewidths=2.0, cmap=sns.color_palette(\"GnBu_d\"));\n    heatmap.yaxis.set_ticklabels(\n        heatmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n    heatmap.xaxis.set_ticklabels(\n        heatmap.xaxis.get_ticklabels(), rotation=45, ha='right')\n    plt.ylabel('References')\n    plt.xlabel('Predictions')\n    return fig\n\ndef process_drop_columns(df, columns):    \n    new_df = df.drop(columns=columns, axis=1, errors='ignore')\n    return new_df\n\ndef proces_boolean(df, columns):\n    for col in columns:\n        df[col] = [1 if x is True else 0 for x in df[col]]\n    return df\n\ndef process_categorical(df, columns):         \n    le = LabelEncoder()\n    label_object = {}\n    for col in columns:\n        labelencoder = LabelEncoder()\n        labelencoder.fit(df[col])\n        df[col] = labelencoder.fit_transform(df[col])\n        label_object[col] = labelencoder\n    return df\n\ndef create_hash(df):\n    df['Hash'] = df['Surname'].apply(lambda x: hash(tuple(x)))\n    return df\n\ndef processCabin(df):\n    df['Cabin'].replace('\\/\\d+\\/', '/', regex=True, inplace=True)\n    return df\n\ndef processName(df):\n    df['Surname'] = df['Name'].apply(lambda item: item.split(\" \")[1])\n    return df\n\ndef createTrainEvalSets(df):\n    df1 = df[np.mod(np.abs(df['Name'].apply(lambda item: hash(item.split(\" \")[1]))), 4) < 3]\n    df2 = df[np.mod(np.abs(df['Name'].apply(lambda item: hash(item.split(\" \")[1]))), 4) >= 3]\n    return (df1, df2)\n# train_df = processCabin(train_df)\n\n#slower\ndef fillAmenityAmount2(df, columns):    \n    for col in df.T.columns:\n        df.T[col].fillna(9, inplace=True)\n    return df  \n    \n    \ndef displayRowsWithNulls(df, columns):\n    display(df[columns][df[columns].isnull().any(axis=1)])\n\ndef process_normalize_columns(df, columns):\n    for col in columns:\n        df[col]=(df[col]-df[col].min())/(df[col].max()-df[col].min())\n    return df\n\ndef fillAmenityAmount(df, columns, regressor):   \n    df1=df[df[columns].isnull().any(axis=1)][columns]    \n    for index, row in df1.iterrows():              \n        features = df1.loc[index, ~pd.isna(row)]\n        \n        if(len(features) < 4):\n            arr = features.to_numpy()\n            arr = np.append(arr, 0)\n            features = pd.Series(arr).to_numpy()\n        features = np.array(features[..., np.newaxis])     \n        \n        prediction = regressor.predict(features.T)\n        col_name = df1.columns[pd.isna(row)][0]\n        \n        df.loc[index, col_name] = prediction[0]   \n    \n    return df\n\ndef fillHomePlanet(df, regressor):    \n    df1=df[df['HomePlanet'].isna()][['Cabin', 'HomePlanet']]\n    for index, row in df1.iterrows():   \n        \n        features = np.array(df1.loc[index, 'Cabin']) \n        features = features[..., np.newaxis]\n        \n        prediction = regressor.predict(features)\n        \n        df.loc[index, 'HomePlanet'] = np.argmax(prediction[0])   \n    \n    return df\n\ndef displayFacetStatistics(df):\n    fsg = FeatureStatisticsGenerator()\n    dataframes = [\n        {'table': df, 'name': 'trainData'}]\n    spaceshipProto = fsg.ProtoFromDataFrames(dataframes)\n    protostr = base64.b64encode(spaceshipProto.SerializeToString()).decode(\"utf-8\")\n\n\n    HTML_TEMPLATE = \"\"\"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n            <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\">\n            <facets-overview id=\"elem\"></facets-overview>\n            <script>\n              document.querySelector(\"#elem\").protoInput = \"{protostr}\";\n            </script>\"\"\"\n    html = HTML_TEMPLATE.format(protostr=protostr)\n    display(HTML(html))\n    \ndef displayFacetsDive(df):\n    SAMPLE_SIZE = 5000 #@param\n    df.dropna(how=\"any\", axis=0, inplace=True)\n    train_dive = train_df.sample(SAMPLE_SIZE).to_json(orient='records')\n    HTML_TEMPLATE = \"\"\"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n            <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\">\n            <facets-dive id=\"elem\" height=\"600\"></facets-dive>\n            <script>\n              var data = {jsonstr};\n              document.querySelector(\"#elem\").data = data;\n            </script>\"\"\"\n    html = HTML_TEMPLATE.format(jsonstr=train_dive)\n    display(HTML(html))\n    \ndef createRegressor(data, features, label):    \n    class myRegressor(tf.keras.Model):\n        def __init__(self):\n            super().__init__()        \n            self.dense1 = Dense(1, activation='relu')  \n            #self.dense2 = Dense(2, activation='relu')         \n            self.dense3 = Dense(1)\n            self.dropout1 = Dropout(0.4)\n\n        def call(self, inputs, training=False):\n            x = self.dense1(inputs)\n            if(training==False):\n                x = self.dropout1(x, training=training)\n            #x = self.dense2(x)\n            return self.dense3(x)\n        \n    myRegressorModel = myRegressor()\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=10)\n\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', \n                                                     factor=0.001,\n                                                     patience=5, \n                                                     min_lr=1e-10)\n\n    myRegressorModel.compile(optimizer=tf.keras.optimizers.Adam(), \n                             loss=tf.keras.losses.MeanSquaredError(), \n                             metrics=['accuracy'])\n\n    \n    data.dropna(inplace=True)\n    features = data[features]\n    labels = data[label]\n    print('fitting regressor------')\n    history = myRegressorModel.fit(features, labels, epochs=100, callbacks=[early_stopping])\n    \n    return myRegressorModel\n\ndef createCategoricalRegressor(data, features, label):\n    class myCategoricalRegressor(tf.keras.Model):\n        def __init__(self):\n            super().__init__()        \n            self.dense1 = Dense(128, activation='relu')  \n            self.dense2 = Dense(64, activation='relu')         \n            self.dense3 = Dense(len(data[label[0]].unique()), activation='softmax')\n            self.dropout1 = Dropout(0.4)\n\n        def call(self, inputs, training=False):\n            x = self.dense1(inputs)\n            if(training==False):\n                x = self.dropout1(x, training=training)\n            #x = self.dense2(x)\n            return self.dense3(x)\n\n\n    myCategoricalRegressorModel = myCategoricalRegressor()\n\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=10)\n\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', \n                                                     factor=0.001,\n                                                     patience=5, \n                                                     min_lr=1e-10)\n    \n    myCategoricalRegressorModel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), \n                                        loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n                                        metrics=['accuracy'])\n\n    \n    data.dropna(inplace=True)\n    features = data[features].to_numpy()\n    label = data[label].to_numpy()\n    print('fitting categorical regressor------')\n    history = myCategoricalRegressorModel.fit(features, label, epochs= 100, callbacks=[early_stopping])\n    return myCategoricalRegressorModel\n\ndef train_model():\n    class myModel(tf.keras.Model):\n        def __init__(self):\n            super().__init__()        \n            self.dense1 = Dense(32, activation='relu')  \n            self.dense2 = Dense(16, activation='relu')         \n            self.dense3 = Dense(1, activation='sigmoid')\n            self.dropout1 = Dropout(0.4)\n\n        def call(self, inputs, training=False):\n            x = self.dense1(inputs)\n            if(training==False):\n                x = self.dropout1(x, training=training)\n            x = self.dense2(x)        \n            return self.dense3(x)\n\n    model = myModel()\n    data = load_data(BASE_DIR_OUTPUT, 'train.csv')\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=10)\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n    features = data[data.columns[:-1]].to_numpy()\n    labels = data['Transported'].to_numpy()\n\n    history = model.fit(features, labels, epochs= 200, callbacks=[early_stopping])\n    model.save(os.path.join(BASE_DIR_OUTPUT, 'spaceship_model'), save_format='tf')\n\ndef make_predictions():\n    df = load_data(BASE_DIR_OUTPUT, 'test.csv')\n    model = load_model(os.path.join(BASE_DIR_OUTPUT, 'spaceship_model'))\n    predictions = model.predict(df.to_numpy())\n    predictions = np.round(predictions)\n    predictions = [True if pred == 1 else False for pred in predictions]\n    \n    passenger_id = np.array(df.index, ndmin=2)\n    preds = np.array(predictions, ndmin=2)\n    preds = np.concatenate((passenger_id.T, preds.T), axis=1)\n    df = pd.DataFrame(preds)\n    predictions_file = os.path.join(BASE_DIR_OUTPUT, 'predictions.csv')\n    \n    if(os.path.exists(predictions_file)):\n        os.remove(predictions)\n            \n    df.to_csv(path_or_buf=predictions_file, header=['PassengerId', 'Transported'], index=False)\n    \ndef run_preprocess_pipeline(input_name, out_name, test=False):\n    \n    df = load_data(input_name)    \n    \n    df = processCabin(df)\n    df = process_normalize_columns(df, numerical_columns)\n    regressor = createRegressor(df, ['FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'], ['RoomService'])\n    df = fillAmenityAmount(df, amenityColumns, regressor)\n    df = process_categorical(df, categorical_columns)\n    regressor = createCategoricalRegressor(df, ['Cabin'], ['HomePlanet'])\n    df = fillHomePlanet(df, regressor)\n\n    df.fillna(method='ffill', inplace=True)\n\n    \n    df = proces_boolean(df, boolean_columns)\n    if(not test):\n        df = proces_boolean(df, label_column)\n    for col in numerical_columns:\n        df[col] = pd.to_numeric(df[col])   \n\n    df = process_drop_columns(df, drop_columns) \n    df.to_csv(out_name)\n    return df\n    \ndef load_data(root_dir, filename):\n    data = pd.read_csv(os.path.join(root_dir, filename), index_col=0, sep=',')    \n    return data\n\n\ndef run_pipeline(in_name, out_name, test=False):    \n    try:\n        load_data(in_name, out_name)  \n    except:\n        run_data_preprocess_pipeline(in_name, out_name, test)       \n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nBASE_DIR = '/kaggle/input/spaceship-titanic'\ntrain_df = pd.read_csv(os.path.join(BASE_DIR, 'train.csv'), index_col=0)\ntest_df = pd.read_csv(os.path.join(BASE_DIR, 'test.csv'), index_col=0)\ndisplay(train_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"markdown","source":"# Preprocess Data","metadata":{}},{"cell_type":"code","source":"features=['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck','Destination', 'HomePlanet', 'Cabin','CryoSleep', 'VIP']\nlabel_column = ['Transported']\ncategorical_columns = ['Destination', 'HomePlanet', 'Cabin']\nboolean_columns = ['CryoSleep', 'VIP']\ndrop_columns=['Name' ,'Hash']\nnumerical_columns=['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\namenityColumns=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']    \n\ntrain_file = os.path.join(BASE_DIR, 'train.csv')\ntrain_file_processed = os.path.join(BASE_DIR_OUTPUT, 'train.csv')\ntest_file = os.path.join(BASE_DIR, 'test.csv')\ntest_file_processed = os.path.join(BASE_DIR_OUTPUT, 'test.csv')\n\nrun_pipeline(train_file, train_file_processed)\nrun_pipeline(test_file, test_file_processed, True)\ntrain_model()\nmake_predictions()          ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom matplotlib import pyplot as plt\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pandas_to_numpy(data):\n    '''Convert a pandas DataFrame into a Numpy array'''\n  # Drop empty rows.\n    data = data.dropna(how=\"any\", axis=0)\n  # Separate DataFrame into two Numpy arrays.\n    labels = np.array(data['Transported'])\n    features = data.drop('Transported', axis=1)\n    features = {name:np.array(value) for name, value in features.items()}\n  \n    return features, labels\n\n#@title Visualize Binary Confusion Matrix and Compute Evaluation Metrics Per Subgroup\nCATEGORY  =  \"HomePlanet\" #@param {type:\"string\"}\nSUBGROUP =  1 #@param {type:\"int\"}\n\n# Labels for annotating axes in plot.\nclasses = ['Transported', 'Not Transported']\n\n# Given define subgroup, generate predictions and obtain its corresponding \n# ground truth.\nsubgroup_filter  = train_df.loc[train_df[CATEGORY] == SUBGROUP]\nfeatures, labels = pandas_to_numpy(subgroup_filter)\n\nsubgroup_results = model.evaluate(x=features, y=labels, verbose=0)\ndisplay(subgroup_filter)\nconfusion_matrix = np.array([[subgroup_results[1], subgroup_results[4]], \n                             [subgroup_results[2], subgroup_results[3]]])\n\nsubgroup_performance_metrics = {\n    'ACCURACY': subgroup_results[5],\n    'PRECISION': subgroup_results[6], \n    'RECALL': subgroup_results[7],\n    'AUC': subgroup_results[8]\n}\nperformance_df = pd.DataFrame(subgroup_performance_metrics, index=[SUBGROUP])\npd.options.display.float_format = '{:,.4f}'.format\n\nplot_confusion_matrix(confusion_matrix, classes, SUBGROUP);\nperformance_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}