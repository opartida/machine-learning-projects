{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow-data-validation\n",
    "%pip install -q tensorflow_data_validation[visualization]\n",
    "%pip install tfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tempfile\n",
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow as tf\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "print('TFDV version: {}'.format(tfdv.version.__version__))\n",
    "print('TF version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and display data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = './data'\n",
    "TRAIN_DATA = os.path.join(DATA, 'train.csv')\n",
    "TEST_DATA = os.path.join(DATA, 'test.csv')\n",
    "OUTPUT = './output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_DATA, sep=\";\")    \n",
    "test_df = pd.read_csv(TEST_DATA, sep=\";\")\n",
    "display(train_df)\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import tensorflow_data_validation as tfdv\n",
    "print('TFDV version: {}'.format(tfdv.version.__version__))\n",
    "train_stats = tfdv.generate_statistics_from_dataframe(train_df)\n",
    "test_stats = tfdv.generate_statistics_from_dataframe(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(train_stats)\n",
    "tfdv.visualize_statistics(lhs_statistics=train_stats,\n",
    "                         rhs_statistics=test_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infer schema and detect anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = tfdv.infer_schema(train_stats)\n",
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "# Create schema environments and remove the label from the testing environment so it is not detected as an anomaly in the test set\n",
    "schema.default_environment.append('TRAINING')\n",
    "schema.default_environment.append('TESTING')\n",
    "\n",
    "tfdv.get_feature(schema, 'EXTRA_BAGGAGE').not_in_environment.append('TESTING')\n",
    "\n",
    "# Generate new statistics based on schema\n",
    "stats_options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True)\n",
    "stats_options.label_feature = 'EXTRA_BAGGAGE'\n",
    "train_stats = tfdv.generate_statistics_from_dataframe(\n",
    "    train_df,\n",
    "    stats_options=stats_options,\n",
    ")\n",
    "\n",
    "# Check for anomalies in the test statistics\n",
    "anomalies = tfdv.validate_statistics(test_stats, schema, environment='TESTING')\n",
    "tfdv.display_anomalies(anomalies)\n",
    "\n",
    "#options = tfdv.StatsOptions(schema=schema)\n",
    "#anomalous_example_stats = tfdv.validate_examples_in_csv(data_location=TRAIN_DATA, stats_options=options)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking data skew and drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.get_feature(schema, 'WEBSITE').skew_comparator.infinity_norm.threshold = 0.01\n",
    "skew_anomalies = tfdv.validate_statistics(statistics=train_stats, schema=schema, serving_statistics=test_stats)\n",
    "tfdv.display_anomalies(skew_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate statistics on data slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_data_validation.utils import slicing_util\n",
    "slice_fn =  slicing_util.get_feature_value_slicer(features={'DEVICE': 'COMPUTER'})\n",
    "stats_options = tfdv.StatsOptions(slice_functions=[slice_fn])\n",
    "\n",
    "train_stats = tfdv.generate_statistics_from_dataframe(\n",
    "    train_df,\n",
    "    stats_options=stats_options,\n",
    ")\n",
    "\n",
    "tfdv.visualize_statistics(train_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U tensorflow-transform\n",
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PIPELINE_NAME = \"extra-baggage\"\n",
    "DATA_ROOT = \"train-data\"\n",
    "\n",
    "# Output directory to store artifacts generated from the pipeline.\n",
    "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
    "# Path to a SQLite DB file to use as an MLMD storage.\n",
    "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
    "\n",
    "from absl import logging\n",
    "\n",
    "logging.set_verbosity(logging.FATAL)  # Set default logging level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "_transform_module_file = 'transform.py' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting transform.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile {_transform_module_file}\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "  \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "\n",
    "  Args:\n",
    "    inputs: map from feature keys to raw not-yet-transformed features.\n",
    "\n",
    "  Returns:\n",
    "    Map from string feature key to transformed feature operations.\n",
    "  \"\"\"\n",
    "  \n",
    "  # Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\n",
    "  _VOCAB_SIZE = 1000\n",
    "  # Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\n",
    "  _OOV_SIZE = 10\n",
    "  # Number of buckets used by tf.transform for encoding each feature.\n",
    "  _FEATURE_BUCKET_COUNT = 10\n",
    "\n",
    "  _FEATURE_KEYS = ['DEPARTURE','ARRIVAL']\n",
    "\n",
    "  _VOCAB_FEATURE_KEYS = ['DEPARTURE', 'ARRIVAL', 'EXTRA_BAGGAGE']\n",
    "\n",
    "  _CATEGORICAL_FEATURE_KEYS = []\n",
    "\n",
    "  _DENSE_FLOAT_FEATURE_KEYS = []\n",
    "\n",
    "  _BUCKET_FEATURE_KEYS = []\n",
    "\n",
    "  _LABEL_KEY = 'EXTRA_BAGGAGE'\n",
    "  outputs = {}\n",
    "  for key in _DENSE_FLOAT_FEATURE_KEYS:\n",
    "    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.\n",
    "    outputs[key] = tft.scale_to_z_score(\n",
    "        _fill_in_missing(inputs[key]))\n",
    "\n",
    "  for key in _VOCAB_FEATURE_KEYS:\n",
    "    # Build a vocabulary for this feature.\n",
    "    outputs[key] = tft.compute_and_apply_vocabulary(\n",
    "            inputs[key],\n",
    "            top_k=_VOCAB_SIZE,\n",
    "            num_oov_buckets=_OOV_SIZE)\n",
    "\n",
    "  for key in _BUCKET_FEATURE_KEYS:\n",
    "    outputs[key] = tft.bucketize(\n",
    "              inputs[key], \n",
    "              _FEATURE_BUCKET_COUNT)\n",
    "\n",
    "  for key in _CATEGORICAL_FEATURE_KEYS:\n",
    "    outputs[key] = inputs[key]  \n",
    "\n",
    "  return outputs  \n",
    "    \n",
    "\n",
    "def _fill_in_missing(x):\n",
    "  \"\"\"Replace missing values in a SparseTensor.\n",
    "  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n",
    "  Args:\n",
    "    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
    "      in the second dimension.\n",
    "  Returns:\n",
    "    A rank 1 tensor where missing values of `x` have been filled in.\n",
    "  \"\"\"\n",
    "  if not isinstance(x, tf.sparse.SparseTensor):\n",
    "    return x\n",
    "\n",
    "  default_value = '' if x.dtype == tf.string else 0\n",
    "  return tf.squeeze(\n",
    "      tf.sparse.to_dense(\n",
    "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
    "          default_value),\n",
    "      axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "_trainer_module_file = 'extra_baggage_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting extra_baggage_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_trainer_module_file}\n",
    "\n",
    "from typing import List\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow import keras\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "import tensorflow as tf\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "from tfx.components.trainer.fn_args_utils import DataAccessor\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "from tfx_bsl.tfxio import dataset_options\n",
    "\n",
    "_TRAIN_BATCH_SIZE = 20\n",
    "_EVAL_BATCH_SIZE = 10\n",
    "\n",
    "_FEATURE_KEYS = ['DEPARTURE', 'ARRIVAL']\n",
    "\n",
    "_LABEL_KEY = 'EXTRA_BAGGAGE'\n",
    "\n",
    "\n",
    "def _apply_preprocessing(raw_features, tft_layer):\n",
    "  transformed_features = tft_layer(raw_features)\n",
    "  if _LABEL_KEY in raw_features:\n",
    "    transformed_label = transformed_features.pop(_LABEL_KEY)\n",
    "    return transformed_features, transformed_label\n",
    "  else:\n",
    "    return transformed_features, None\n",
    "\n",
    "def _get_serve_rest_fn(model, tf_transform_output):\n",
    "  \n",
    "  model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "  @tf.function(input_signature=[\n",
    "      tf.TensorSpec(shape=(None,1), dtype=tf.string, name='departure'),\n",
    "      tf.TensorSpec(shape=(None,1), dtype=tf.string, name='arrival'),\n",
    "  ])\n",
    "  def serve_rest_fn(x0, x1):\n",
    "    # Run inference with ML model.    \n",
    "    transformed_features, _ = _apply_preprocessing({\n",
    "                                                  'DEPARTURE': x0,\n",
    "                                                  'ARRIVAL': x1,\n",
    "                                                  },\n",
    "                                                   model.tft_layer)\n",
    "    print(transformed_features)\n",
    "    return model(transformed_features)\n",
    "\n",
    "  return serve_rest_fn\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "  \n",
    "  model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "  @tf.function(input_signature=[\n",
    "      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
    "  ])\n",
    "  def serve_tf_examples_fn(serialized_tf_examples):\n",
    "    # Expected input is a string which is serialized tf.Example format.\n",
    "    feature_spec = tf_transform_output.raw_feature_spec()\n",
    "    \n",
    "    # Because input schema includes unnecessary fields like 'species' and\n",
    "    # 'island', we filter feature_spec to include required keys only.\n",
    "    required_feature_spec = {\n",
    "        k: v for k, v in feature_spec.items() if k in _FEATURE_KEYS\n",
    "    }\n",
    "    parsed_features = tf.io.parse_example(serialized_tf_examples,\n",
    "                                          required_feature_spec)\n",
    "\n",
    "    # Preprocess parsed input with transform operation defined in\n",
    "    # preprocessing_fn().\n",
    "    transformed_features, _ = _apply_preprocessing(parsed_features,\n",
    "                                                   model.tft_layer)\n",
    "    # Run inference with ML model.\n",
    "    return model(transformed_features)\n",
    "\n",
    "  return serve_tf_examples_fn\n",
    "\n",
    "    \n",
    "def _input_fn(file_pattern: List[str],\n",
    "              data_accessor: DataAccessor,\n",
    "              tf_transform_output: tft.TFTransformOutput,\n",
    "              batch_size: int = 200) -> tf.data.Dataset:\n",
    "  \"\"\"Generates features and label for training.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "    schema: schema of the input data.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "  Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "  \"\"\"\n",
    "  \n",
    "  dataset = data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      dataset_options.TensorFlowDatasetOptions(\n",
    "          batch_size=batch_size),\n",
    "      tf_transform_output.raw_metadata.schema).repeat()\n",
    "\n",
    "  transform_layer = tf_transform_output.transform_features_layer()\n",
    "  \n",
    "  def apply_transform(raw_features):    \n",
    "    return _apply_preprocessing(raw_features, transform_layer)\n",
    "\n",
    "  return dataset.map(apply_transform).repeat()\n",
    "\n",
    "\n",
    "def _build_keras_model() -> tf.keras.Model:\n",
    "  \"\"\"Creates a DNN Keras model for classifying booking data.\n",
    "\n",
    "  Returns:\n",
    "    A Keras Model.\n",
    "  \"\"\"\n",
    "  # The model below is built with Functional API, please refer to\n",
    "  # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
    "  inputs = [keras.layers.Input(shape=(1,1), name=f) for f in _FEATURE_KEYS]\n",
    "  d = keras.layers.concatenate(inputs)\n",
    "  for _ in range(2):\n",
    "    d = keras.layers.Dense(8, activation='relu')(d)\n",
    "  outputs = keras.layers.Dense(1, activation='sigmoid')(d)\n",
    "\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "  model.compile(\n",
    "      optimizer=keras.optimizers.Adam(1e-3),\n",
    "      loss=keras.losses.BinaryCrossentropy(),\n",
    "      metrics=[keras.metrics.Accuracy()])\n",
    "\n",
    "  model.summary(print_fn=logging.info)\n",
    "  return model\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: FnArgs):\n",
    "  \"\"\"Train the model based on given args.\n",
    "\n",
    "  Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "  \"\"\"  \n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "  \n",
    "  train_dataset = _input_fn(\n",
    "      fn_args.train_files,\n",
    "      fn_args.data_accessor,\n",
    "      tf_transform_output,\n",
    "      batch_size=_TRAIN_BATCH_SIZE)  \n",
    "\n",
    "  model = _build_keras_model()\n",
    "  model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      validation_steps=fn_args.eval_steps,\n",
    "      callbacks=[tensorboard_callback])\n",
    "\n",
    "  # The result of the training should be saved in `fn_args.serving_model_dir`\n",
    "  # directory.\n",
    " \n",
    "  signatures = {\n",
    "      'serving_default': _get_serve_tf_examples_fn(model, tf_transform_output),\n",
    "      'serving_rest': _get_serve_rest_fn(model, tf_transform_output),\n",
    "  }\n",
    "  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx import v1 as tfx\n",
    "def _import_schema(schema_path=''):\n",
    "  tfx.dsl.Importer(\n",
    "      source_uri=schema_path,\n",
    "      artifact_type=tfx.types.standard_artifacts.Schema).with_id(\n",
    "          'schema_importer')\n",
    "\n",
    "def _create_pipeline(pipeline_name: str, \n",
    "                     pipeline_root: str, \n",
    "                     data_root: str, \n",
    "                     schema_path: str,\n",
    "                     module_file: str, \n",
    "                     transform_module_file: str, \n",
    "                     serving_model_dir: str,\n",
    "                     metadata_path: str) -> tfx.dsl.Pipeline:\n",
    "\n",
    "  \"\"\"Creates a pipeline with TFX.\"\"\"\n",
    "  # Brings data into the pipeline.\n",
    "  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "  stats_gen = tfx.components.StatisticsGen(examples=example_gen.outputs['examples'])\n",
    "\n",
    "  #schema_importer = tfx.dsl.Importer(\n",
    "  #    source_uri=schema_path,\n",
    "  #    artifact_type=tfx.types.standard_artifacts.Schema).with_id(\n",
    "  #        'schema_importer')\n",
    "  schema_importer= tfx.components.SchemaGen(\n",
    "      statistics=stats_gen.outputs['statistics'], infer_feature_shape=True)\n",
    "\n",
    "  example_validator = tfx.components.ExampleValidator(\n",
    "    statistics=stats_gen.outputs['statistics'],\n",
    "    schema=schema_importer.outputs['schema'])\n",
    "\n",
    "  transform = tfx.components.Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_importer.outputs['schema'],\n",
    "    module_file=_transform_module_file)\n",
    "\n",
    "  # Uses user-provided Python function that trains a model.\n",
    "  trainer = tfx.components.Trainer(\n",
    "      module_file=module_file,\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      schema=schema_importer.outputs['schema'],\n",
    "      transform_graph=transform.outputs['transform_graph'],\n",
    "      train_args=tfx.proto.TrainArgs(num_steps=100),\n",
    "      eval_args=tfx.proto.EvalArgs(num_steps=5))  \n",
    "\n",
    "  # Pushes the model to a filesystem destination.\n",
    "  pusher = tfx.components.Pusher(\n",
    "      model=trainer.outputs['model'],\n",
    "      push_destination=tfx.proto.PushDestination(\n",
    "          filesystem=tfx.proto.PushDestination.Filesystem(\n",
    "              base_directory=serving_model_dir)))\n",
    "\n",
    "  # Following three components will be included in the pipeline.\n",
    "  components = [\n",
    "      example_gen,\n",
    "      stats_gen,\n",
    "      schema_importer,\n",
    "      example_validator,\n",
    "\n",
    "      transform,  # NEW: Transform component was added to the pipeline.\n",
    "\n",
    "      trainer,\n",
    "      pusher,\n",
    "  ]\n",
    "\n",
    "  return tfx.dsl.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      metadata_connection_config=tfx.orchestration.metadata\n",
    "      .sqlite_metadata_connection_config(metadata_path),\n",
    "      components=components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying extra_baggage_trainer.py -> build/lib\n",
      "copying transform.py -> build/lib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing to /var/folders/tz/gyhk2p3j6hx4bmdqw8bkql840000gn/T/tmpxez9e0st\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/extra_baggage_trainer.py -> /var/folders/tz/gyhk2p3j6hx4bmdqw8bkql840000gn/T/tmpxez9e0st\n",
      "copying build/lib/transform.py -> /var/folders/tz/gyhk2p3j6hx4bmdqw8bkql840000gn/T/tmpxez9e0st\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Transform.egg-info\n",
      "writing tfx_user_code_Transform.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Transform.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Transform.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Transform.egg-info to /var/folders/tz/gyhk2p3j6hx4bmdqw8bkql840000gn/T/tmpxez9e0st/tfx_user_code_Transform-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986-py3.9.egg-info\n",
      "running install_scripts\n",
      "creating /var/folders/tz/gyhk2p3j6hx4bmdqw8bkql840000gn/T/tmpxez9e0st/tfx_user_code_Transform-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986.dist-info/WHEEL\n",
      "creating '/var/folders/tz/gyhk2p3j6hx4bmdqw8bkql840000gn/T/tmp565zziqg/tfx_user_code_Transform-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986-py3-none-any.whl' and adding '/var/folders/tz/gyhk2p3j6hx4bmdqw8bkql840000gn/T/tmpxez9e0st' to it\n",
      "adding 'extra_baggage_trainer.py'\n",
      "adding 'transform.py'\n",
      "adding 'tfx_user_code_Transform-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Transform-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Transform-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Transform-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986.dist-info/RECORD'\n",
      "removing /var/folders/tz/gyhk2p3j6hx4bmdqw8bkql840000gn/T/tmpxez9e0st\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying extra_baggage_trainer.py -> build/lib\n",
      "copying transform.py -> build/lib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing to /var/folders/tz/gyhk2p3j6hx4bmdqw8bkql840000gn/T/tmpv6hh9vps\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/extra_baggage_trainer.py -> /var/folders/tz/gyhk2p3j6hx4bmdqw8bkql840000gn/T/tmpv6hh9vps\n",
      "copying build/lib/transform.py -> /var/folders/tz/gyhk2p3j6hx4bmdqw8bkql840000gn/T/tmpv6hh9vps\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Trainer.egg-info\n",
      "writing tfx_user_code_Trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Trainer.egg-info to /var/folders/tz/gyhk2p3j6hx4bmdqw8bkql840000gn/T/tmpv6hh9vps/tfx_user_code_Trainer-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986-py3.9.egg-info\n",
      "running install_scripts\n",
      "creating /var/folders/tz/gyhk2p3j6hx4bmdqw8bkql840000gn/T/tmpv6hh9vps/tfx_user_code_Trainer-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986.dist-info/WHEEL\n",
      "creating '/var/folders/tz/gyhk2p3j6hx4bmdqw8bkql840000gn/T/tmpdn4remll/tfx_user_code_Trainer-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986-py3-none-any.whl' and adding '/var/folders/tz/gyhk2p3j6hx4bmdqw8bkql840000gn/T/tmpv6hh9vps' to it\n",
      "adding 'extra_baggage_trainer.py'\n",
      "adding 'transform.py'\n",
      "adding 'tfx_user_code_Trainer-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Trainer-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Trainer-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Trainer-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986.dist-info/RECORD'\n",
      "removing /var/folders/tz/gyhk2p3j6hx4bmdqw8bkql840000gn/T/tmpv6hh9vps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0819 19:09:30.759780000 4701634048 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./pipelines/extra-baggage/_wheels/tfx_user_code_Transform-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-Transform\n",
      "Successfully installed tfx-user-code-Transform-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./pipelines/extra-baggage/_wheels/tfx_user_code_Transform-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-Transform\n",
      "Successfully installed tfx-user-code-Transform-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./pipelines/extra-baggage/_wheels/tfx_user_code_Transform-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-Transform\n",
      "Successfully installed tfx-user-code-Transform-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n",
      "WARNING:root:This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType], int] instead.\n",
      "WARNING:root:This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType], int] instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: pipelines/extra-baggage/Transform/transform_graph/969/.temp_path/tftransform_tmp/7a77702d8c4c457fb11aef2ccf49d529/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: pipelines/extra-baggage/Transform/transform_graph/969/.temp_path/tftransform_tmp/7a77702d8c4c457fb11aef2ccf49d529/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: pipelines/extra-baggage/Transform/transform_graph/969/.temp_path/tftransform_tmp/224b431bd69b4e38b58aa26219d5e556/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: pipelines/extra-baggage/Transform/transform_graph/969/.temp_path/tftransform_tmp/224b431bd69b4e38b58aa26219d5e556/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n",
      "E0819 19:10:55.098000000 4701634048 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./pipelines/extra-baggage/_wheels/tfx_user_code_Trainer-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-Trainer\n",
      "Successfully installed tfx-user-code-Trainer-0.0+1ff575ff9b7ee2eedd55ab03c1bcfd9b80ddc148453b0fd9f02084e89aa76986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s 3ms/step - loss: 1.0742 - accuracy: 0.0000e+00\n",
      "{'DEPARTURE': <tf.Tensor 'transform_features_layer_26/StatefulPartitionedCall:1' shape=(None, 1) dtype=int64>, 'ARRIVAL': <tf.Tensor 'transform_features_layer_26/StatefulPartitionedCall:0' shape=(None, 1) dtype=int64>}\n",
      "INFO:tensorflow:Assets written to: pipelines/extra-baggage/Trainer/model/970/Format-Serving/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: pipelines/extra-baggage/Trainer/model/970/Format-Serving/assets\n"
     ]
    }
   ],
   "source": [
    "tfx.orchestration.LocalDagRunner().run(\n",
    "  _create_pipeline(\n",
    "      pipeline_name=PIPELINE_NAME,\n",
    "      pipeline_root=PIPELINE_ROOT,\n",
    "      data_root=DATA_ROOT,\n",
    "      schema_path='',\n",
    "      module_file =_trainer_module_file,\n",
    "      transform_module_file =_transform_module_file,\n",
    "      serving_model_dir=SERVING_MODEL_DIR,\n",
    "      metadata_path=METADATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_metadata.proto import metadata_store_pb2\n",
    "# Non-public APIs, just for showcase.\n",
    "from tfx.orchestration.portable.mlmd import execution_lib\n",
    "\n",
    "# TODO(b/171447278): Move these functions into the TFX library.\n",
    "\n",
    "def get_latest_artifacts(metadata, pipeline_name, component_id):\n",
    "  \"\"\"Output artifacts of the latest run of the component.\"\"\"\n",
    "  context = metadata.store.get_context_by_type_and_name(\n",
    "      'node', f'{pipeline_name}.{component_id}')\n",
    "  executions = metadata.store.get_executions_by_context(context.id)\n",
    "  latest_execution = max(executions,\n",
    "                         key=lambda e:e.last_update_time_since_epoch)\n",
    "  return execution_lib.get_artifacts_dict(metadata, latest_execution.id,\n",
    "                                          [metadata_store_pb2.Event.OUTPUT])\n",
    "\n",
    "# Non-public APIs, just for showcase.\n",
    "from tfx.orchestration.experimental.interactive import visualizations\n",
    "\n",
    "def visualize_artifacts(artifacts):\n",
    "  \"\"\"Visualizes artifacts using standard visualization modules.\"\"\"\n",
    "  for artifact in artifacts:\n",
    "    visualization = visualizations.get_registry().get_visualization(\n",
    "        artifact.type_name)\n",
    "    if visualization:\n",
    "      visualization.display(artifact)\n",
    "\n",
    "from tfx.orchestration.experimental.interactive import standard_visualizations\n",
    "standard_visualizations.register_standard_visualizations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-public APIs, just for showcase.\n",
    "from tfx.orchestration.metadata import Metadata\n",
    "from tfx.types import standard_component_specs\n",
    "SCHEMA_METADATA_PATH = METADATA_PATH\n",
    "metadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(\n",
    "    SCHEMA_METADATA_PATH)\n",
    "\n",
    "with Metadata(metadata_connection_config) as metadata_handler:\n",
    "  # Find output artifacts from MLMD.\n",
    "  stat_gen_output = get_latest_artifacts(metadata_handler, PIPELINE_NAME,\n",
    "                                         'StatisticsGen')\n",
    "  stats_artifacts = stat_gen_output[standard_component_specs.STATISTICS_KEY]\n",
    "\n",
    "  schema_gen_output = get_latest_artifacts(metadata_handler,\n",
    "                                           PIPELINE_NAME, 'SchemaGen')\n",
    "  schema_artifacts = schema_gen_output[standard_component_specs.SCHEMA_KEY]\n",
    "\n",
    "  transform_gen_output = get_latest_artifacts(metadata_handler,\n",
    "                                           PIPELINE_NAME, 'Transform')\n",
    "  transform_artifacts = transform_gen_output[standard_component_specs.TRANSFORM_GRAPH_KEY]\n",
    "\n",
    "  trainer_gen_output = get_latest_artifacts(metadata_handler,\n",
    "                                           PIPELINE_NAME, 'Trainer')\n",
    "  trainer_artifacts = trainer_gen_output[standard_component_specs.TRAIN_ARGS_KEY]\n",
    "\n",
    "  #ev_output = get_latest_artifacts(metadata_handler, PIPELINE_NAME,\n",
    "  #                                 'ExampleValidator')\n",
    "  #anomalies_artifacts = ev_output[standard_component_specs.ANOMALIES_KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "visualize_artifacts(trainer_artifacts)\n",
    "print(trainer_artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "_schema_filename = 'schema.pbtxt'\n",
    "SCHEMA_PATH = 'schema'\n",
    "\n",
    "os.makedirs(SCHEMA_PATH, exist_ok=True)\n",
    "_generated_path = os.path.join(schema_artifacts[0].uri, _schema_filename)\n",
    "\n",
    "# Copy the 'schema.pbtxt' file from the artifact uri to a predefined path.\n",
    "shutil.copy(_generated_path, SCHEMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "model_dirs = (item for item in os.scandir(SERVING_MODEL_DIR) if item.is_dir())\n",
    "\n",
    "model_path = max(model_dirs, key=lambda i: int(i.name)).path\n",
    "loaded_model = tf.saved_model.load(model_path)\n",
    "inference_fn = loaded_model.signatures['serving_default']\n",
    "\n",
    "features = {\n",
    "      \"ARRIVAL\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[b\"22/July\"])),\n",
    "      \"DEPARTURE\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[b\"22/July\"])),\n",
    "      \"DISTANCE\": tf.train.Feature(float_list=tf.train.FloatList(value=[3206.92])),\n",
    "      \"ADULTS\": tf.train.Feature(int64_list=tf.train.Int64List(value=[1])),\n",
    "      \"CHILDREN\": tf.train.Feature(int64_list=tf.train.Int64List(value=[0])),\n",
    "      \"INFANTS\": tf.train.Feature(int64_list=tf.train.Int64List(value=[0])),\n",
    "      \"TRAIN\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[b\"FALSE\"])),\n",
    "      \"GDS\": tf.train.Feature(int64_list=tf.train.Int64List(value=[1])),\n",
    "      \"TRIP_TYPE\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[b\"TRIP_TYPE\"])),\n",
    "      \"HAUL_TYPE\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[b\"DOMESTIC\"])),\n",
    "      \"NO_GDS\": tf.train.Feature(int64_list=tf.train.Int64List(value=[0])),\n",
    "      \"WEBSITE\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[b\"EDES\"])),\n",
    "      \"PRODUCT\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[b\"TRIP\"])),\n",
    "      \"SMS\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[b\"FALSE\"])),      \n",
    "      \"DEVICE\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[b\"COMPUTER\"])),\n",
    "    }\n",
    "\n",
    "\n",
    "example_proto = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "examples = example_proto.SerializeToString()\n",
    "examples=tf.constant([examples])\n",
    "result = inference_fn(examples=examples)\n",
    "print(result['output_0'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.00018514]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "model_dirs = (item for item in os.scandir(SERVING_MODEL_DIR) if item.is_dir())\n",
    "\n",
    "model_path = max(model_dirs, key=lambda i: int(i.name)).path\n",
    "loaded_model = tf.saved_model.load(model_path)\n",
    "inference_fn = loaded_model.signatures['serving_rest']\n",
    "departure_t = tf.constant(\"22/jul/22\", dtype=tf.string, shape=(1,1))\n",
    "arrival_t = tf.constant(\"22/jul/22\", dtype=tf.string, shape=(1,1))\n",
    "\n",
    "result = inference_fn(departure=departure_t, arrival=arrival_t)\n",
    "print(result['output_0'].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "try: # detect TPUs\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except ValueError: # detect GPUs\n",
    "  strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n",
    "  #strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
    "  #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n",
    "\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.Variable(['a'], dtype=tf.string, shape=(None,1))\n",
    "print(t.shape)\n",
    "fill_tensor = tf.fill(t.shape, '')\n",
    "print(fill_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "      \"ARRIVAL\": \"22/July\",\n",
    "      \"TRAIN\": \"FALSE\",\n",
    "      \"CHILDREN\": 0,\n",
    "      \"ADULTS\": 1,\n",
    "      \"INFANTS\": 0,\n",
    "      \"GDS\": 1,\n",
    "      \"TRIP_TYPE\": \"TRIP\",\n",
    "      \"DISTANCE\": 3206.92,\n",
    "      \"DEVICE\": \"COMPUTER\",\n",
    "      \"NO_GDS\": 0,\n",
    "      \"HAUL_TYPE\": \"DOMESTIC\",\n",
    "      \"WEBSITE\": \"EDES\",\n",
    "      \"DEPARTURE\": \"22/July\",\n",
    "      \"PRODUCT\": \"TRIP\",\n",
    "      \"SMS\": \"FALSE\",      \n",
    "    }\n",
    "\n",
    "    tf.TensorSpec(shape=[None], dtype=tf.string, name='ARRIVAL'),\n",
    "      tf.TensorSpec(shape=[None], dtype=tf.string, name='TRAIN'),\n",
    "      tf.TensorSpec(shape=[None], dtype=tf.int32, name='CHILDREN'),\n",
    "      tf.TensorSpec(shape=[None], dtype=tf.int32, name='ADULTS'),\n",
    "      tf.TensorSpec(shape=[None], dtype=tf.int32, name='INFANTS'),\n",
    "      tf.TensorSpec(shape=[None], dtype=tf.int32, name='GDS'),\n",
    "      tf.TensorSpec(shape=[None], dtype=tf.string, name='TRIP_TYPE'),\n",
    "      tf.TensorSpec(shape=[None], dtype=tf.float32, name='DISTANCE'),\n",
    "      tf.TensorSpec(shape=[None], dtype=tf.string, name='DEVICE'),\n",
    "      tf.TensorSpec(shape=[None], dtype=tf.int32, name='NO_GDS'),\n",
    "      tf.TensorSpec(shape=[None], dtype=tf.string, name='HAUL_TYPE'),\n",
    "      tf.TensorSpec(shape=[None], dtype=tf.int32, name='WEBSITE'),\n",
    "      tf.TensorSpec(shape=[None], dtype=tf.string, name='DEPARTURE'),\n",
    "      tf.TensorSpec(shape=[None], dtype=tf.string, name='PRODUCT'),\n",
    "      tf.TensorSpec(shape=[None], dtype=tf.string, name='SMS'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tf.train.Feature(bytes_list=tf.train.BytesList(value=[b\"22/July\"]))\n",
    "features = {\n",
    "    'ARRIVAL': tf.train.Feature(bytes_list=tf.train.BytesList(value=[b\"22/July\"]))\n",
    "}\n",
    "features=tf.train.Features(feature=features)\n",
    "example = tf.train.Example(features=features)\n",
    "\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
